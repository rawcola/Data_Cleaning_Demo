{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import collections\n",
    "import numpy as np\n",
    "import copy\n",
    "cancer = load_breast_cancer()\n",
    "# X, y = cancer.data, cancer.target\n",
    "# print(cancer.target.dtype)\n",
    "# print(np.sum(y==0))\n",
    "# print(np.sum(y==1))\n",
    "\n",
    "# class_0 = X[y==0]\n",
    "# class_1 = X[y==1]\n",
    "\n",
    "# class_0 = np.concatenate((class_0, np.zeros((class_0.shape[0], 1))), axis=1)\n",
    "# class_1 = np.concatenate((class_1, np.ones((class_1.shape[0], 1))), axis=1)\n",
    "\n",
    "# np.random.shuffle(class_0)\n",
    "# np.random.shuffle(class_1)\n",
    "\n",
    "# train_0 = class_0[0:100]\n",
    "# train_1 = class_1[0:100]\n",
    "# noise_data_0 = class_0[100:]\n",
    "# noise_data_1 = class_1[100:]\n",
    "# noise_size = 30\n",
    "# noise_data_0, noise_data_1 = np.concatenate((noise_data_1[:noise_size], noise_data_0[noise_size:]),axis=0), np.concatenate((noise_data_0[:noise_size], noise_data_1[noise_size:]),axis=0)\n",
    "\n",
    "def make_datasets(raw_data):\n",
    "  X, y = raw_data.data, raw_data.target\n",
    "  gt_labels = np.unique(y).tolist()\n",
    "  noise_size = 30\n",
    "  noisy_data = np.array([])\n",
    "  train_data = {}\n",
    "  noise_data = {}\n",
    "  for label in gt_labels:\n",
    "    class_ = X[y==label]\n",
    "    class_ = np.concatenate((class_, label*np.ones((class_.shape[0],1))), axis=1)\n",
    "    train_data[label] = class_[:100]\n",
    "    tmp = class_[100:]\n",
    "    noise_data[label] = tmp[noise_size:]\n",
    "    try:\n",
    "      noisy_data = np.concatenate((noisy_data,tmp[:noise_size]), axis=0)\n",
    "    except:\n",
    "      noisy_data = tmp[:noise_size]\n",
    "  np.random.shuffle(noisy_data)\n",
    "  cnt = 0\n",
    "  for label, _ in noise_data.items():\n",
    "    noise_data[label] = np.concatenate((noise_data[label],noisy_data[cnt*noise_size: (cnt + 1)*noise_size]), axis=0)\n",
    "    cnt += 1\n",
    "  return train_data, noise_data, gt_labels\n",
    "\n",
    "def default_array():\n",
    "  return np.array([])\n",
    "\n",
    "def data_cleaning(train_data, noise_data, gt_labels, clf, epoch, contamination=0.2):\n",
    "  cleaned_data = collections.defaultdict(default_array)\n",
    "  cost = 0\n",
    "  for e in range(epoch):\n",
    "    print(f'now epoch: {e}...')\n",
    "    # 使用已有或经过扩充的数据集对待清洗数据集进行异常检测，异常数据假设使用正确率百分之百的人工标注\n",
    "    for key, val in train_data.items():\n",
    "      clf.fit(val[:,:-1])\n",
    "      noise_data_ = noise_data[key]\n",
    "      labels = clf.predict(noise_data_[:,:-1])\n",
    "      gt = noise_data_[:,-1] != key\n",
    "      pred = labels==-1\n",
    "      tp = np.sum(gt&pred)\n",
    "      fp = np.sum(gt&(~pred))\n",
    "      fn = np.sum((~gt)&pred)\n",
    "      tn = np.sum((~gt)&(~pred))\n",
    "      percesion = -1 if tp + fp == 0 else tp / (tp + fp)\n",
    "      recall = -1 if tp + fn == 0 else tp / (tp + fn)\n",
    "      print(f'anomal data, class {key}: tp:{tp}, fp:{fp}, fn:{fn}, tn:{tn}, precision:{percesion:.2f}, recall:{recall:.2f}')\n",
    "      annotation_data = noise_data_[labels==-1]\n",
    "      anno = annotation_data[:,-1]\n",
    "      cost += anno.shape[0]\n",
    "      # 更新到已经清洗过的数据集\n",
    "      for i in gt_labels:\n",
    "        try:\n",
    "          cleaned_data[i] = np.concatenate((cleaned_data[i], annotation_data[anno==i]), axis=0)\n",
    "        except:\n",
    "          cleaned_data[i] = annotation_data[anno==i]\n",
    "      noise_data[key] = noise_data_[labels==1]\n",
    "    # 使用经过人工标注的数据扩充训练集\n",
    "    for key, val in cleaned_data.items():\n",
    "      train_data[key] = np.concatenate((train_data[key], val), axis=0)\n",
    "    # 统计经过一轮清洗后各类别数据的质量\n",
    "    for key, val in cleaned_data.items():\n",
    "      tmp = np.concatenate((noise_data[key], val), axis=0)\n",
    "      tp = np.sum(tmp[:,-1] == key)\n",
    "      print(f'cleaned data, class: {key}, tp:{tp}, total: {tmp.shape[0]}, precision: {tp / tmp.shape[0]:.2f}')\n",
    "  print(f'cost: {cost}')\n",
    "\n",
    "contamination = 0.10\n",
    "LOF_clf = LocalOutlierFactor(n_neighbors=20, contamination=contamination, novelty=True)\n",
    "EE_clf = EllipticEnvelope(contamination=contamination)\n",
    "IF_clf = IsolationForest(n_estimators=100, max_samples='auto', contamination=contamination)\n",
    "\n",
    "def compute_percesion(data):\n",
    "  for key, val in data.items():\n",
    "    tp = np.sum(val[:,-1]==key)\n",
    "    print(f'class: {key}, tp: {tp}, total: {val.shape[0]}, precision: {tp / val.shape[0]:.2f}')\n",
    "\n",
    "train_data, noise_data, gt_labels = make_datasets(cancer)\n",
    "\n",
    "print('LOF_clf')\n",
    "compute_percesion(noise_data)\n",
    "data_cleaning(train_data.copy(), noise_data.copy(), gt_labels.copy(), LOF_clf, 2)\n",
    "print('-----------------------------------')\n",
    "\n",
    "print('EE_clf')\n",
    "compute_percesion(noise_data)\n",
    "data_cleaning(train_data.copy(), noise_data.copy(), gt_labels.copy(), EE_clf, 2)\n",
    "print('-----------------------------------')\n",
    "\n",
    "print('IF_clf')\n",
    "compute_percesion(noise_data)\n",
    "data_cleaning(train_data.copy(), noise_data.copy(), gt_labels.copy(), IF_clf, 2)\n",
    "print('-----------------------------------')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
